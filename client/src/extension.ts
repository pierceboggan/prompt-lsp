import * as path from 'path';
import * as vscode from 'vscode';
import {
  LanguageClient,
  LanguageClientOptions,
  ServerOptions,
  TransportKind,
  RequestType,
} from 'vscode-languageclient/node';

interface LLMProxyRequest {
  prompt: string;
  systemPrompt: string;
}

interface LLMProxyResponse {
  text: string;
  error?: string;
}

const LLMRequestType = new RequestType<LLMProxyRequest, LLMProxyResponse, void>('promptLSP/llmRequest');

let client: LanguageClient;
let outputChannel: vscode.OutputChannel;
let cachedModel: vscode.LanguageModelChat | undefined;
let modelSelectionPromise: Promise<vscode.LanguageModelChat | undefined> | undefined;

export function activate(context: vscode.ExtensionContext) {
  outputChannel = vscode.window.createOutputChannel('Prompt LSP');

  // Path to the server module
  const serverModule = context.asAbsolutePath(path.join('..', 'out', 'server.js'));

  // Debug options for the server
  const debugOptions = { execArgv: ['--nolazy', '--inspect=6009'] };

  // Server options - run the server module
  const serverOptions: ServerOptions = {
    run: { module: serverModule, transport: TransportKind.ipc },
    debug: {
      module: serverModule,
      transport: TransportKind.ipc,
      options: debugOptions,
    },
  };

  // Client options
  const clientOptions: LanguageClientOptions = {
    // Register the server for prompt documents
    documentSelector: [
      { scheme: 'file', language: 'prompt' },
      { scheme: 'file', pattern: '**/*.prompt.md' },
      { scheme: 'file', pattern: '**/*.system.md' },
      { scheme: 'file', pattern: '**/*.agent.md' },
      { scheme: 'file', pattern: '**/*.prompt' },
      // Also support markdown files with certain patterns
      { scheme: 'file', language: 'markdown', pattern: '**/prompts/**/*.md' },
      { scheme: 'file', language: 'markdown', pattern: '**/skills/**/*.md' },
    ],
    synchronize: {
      // Notify the server about file changes to prompt files
      fileEvents: [
        vscode.workspace.createFileSystemWatcher('**/*.{prompt.md,system.md,agent.md,prompt}'),
        vscode.workspace.createFileSystemWatcher('**/skills/**/*.md'),
      ],
    },
    outputChannel,
  };

  // Create the language client
  client = new LanguageClient(
    'promptLSP',
    'Prompt LSP',
    serverOptions,
    clientOptions
  );

  // Register the LLM proxy handler — the server will send requests here
  client.onRequest(LLMRequestType, async (request: LLMProxyRequest): Promise<LLMProxyResponse> => {
    outputChannel.appendLine('[LLM Proxy] Received request from server');
    const result = await handleLLMProxyRequest(request);
    if (result.error) {
      outputChannel.appendLine(`[LLM Proxy] Error: ${result.error}`);
    } else {
      outputChannel.appendLine(`[LLM Proxy] Success (${result.text.length} chars)`);
    }
    return result;
  });

  // Register commands
  context.subscriptions.push(
    vscode.commands.registerCommand('promptLSP.analyzePrompt', () => {
      const editor = vscode.window.activeTextEditor;
      if (editor) {
        // Send notification to server to trigger full analysis
        client.sendNotification('promptLSP/analyze', { uri: editor.document.uri.toString() });
        vscode.window.showInformationMessage('Running prompt analysis (including LLM)...');
      }
    })
  );

  context.subscriptions.push(
    vscode.commands.registerCommand('promptLSP.showTokenCount', async () => {
      const editor = vscode.window.activeTextEditor;
      if (editor) {
        const text = editor.document.getText();
        // Rough estimation: ~4 characters per token for English
        const estimatedTokens = Math.ceil(text.length / 4);
        vscode.window.showInformationMessage(
          `Estimated tokens: ~${estimatedTokens} (${text.length} characters)`
        );
      }
    })
  );

  context.subscriptions.push(
    vscode.commands.registerCommand('promptLSP.clearCache', () => {
      // Send notification to server to clear cache
      client.sendNotification('promptLSP/clearCache');
      vscode.window.showInformationMessage('Analysis cache cleared.');
    })
  );

  // Create status bar item for token count
  const tokenStatusBar = vscode.window.createStatusBarItem(
    vscode.StatusBarAlignment.Right,
    100
  );
  tokenStatusBar.command = 'promptLSP.showTokenCount';
  context.subscriptions.push(tokenStatusBar);

  // Update token count on active editor change
  const updateTokenCount = () => {
    const editor = vscode.window.activeTextEditor;
    if (editor && isPromptDocument(editor.document)) {
      const text = editor.document.getText();
      const estimatedTokens = Math.ceil(text.length / 4);
      tokenStatusBar.text = `$(symbol-number) ~${estimatedTokens} tokens`;
      tokenStatusBar.tooltip = 'Estimated token count (click for details)';
      tokenStatusBar.show();
    } else {
      tokenStatusBar.hide();
    }
  };

  context.subscriptions.push(
    vscode.window.onDidChangeActiveTextEditor(updateTokenCount)
  );
  context.subscriptions.push(
    vscode.workspace.onDidChangeTextDocument((e) => {
      if (vscode.window.activeTextEditor?.document === e.document) {
        updateTokenCount();
      }
    })
  );

  // Invalidate cached model when available models change
  if (vscode.lm && vscode.lm.onDidChangeChatModels) {
    context.subscriptions.push(
      vscode.lm.onDidChangeChatModels(() => {
        outputChannel.appendLine('[LLM Proxy] Models changed, clearing cache');
        cachedModel = undefined;
        modelSelectionPromise = undefined;
      })
    );
  }

  // Start the client
  client.start();

  // Initial update
  updateTokenCount();

  console.log('Prompt LSP extension activated');
}

/**
 * Handle LLM proxy requests from the language server using vscode.lm API.
 * This lets the extension use the user's Copilot subscription instead of requiring API keys.
 */
async function selectModel(): Promise<vscode.LanguageModelChat | undefined> {
  if (cachedModel) {
    return cachedModel;
  }

  // If another call is already selecting, wait for it
  if (modelSelectionPromise) {
    return modelSelectionPromise;
  }

  modelSelectionPromise = doSelectModel();
  try {
    return await modelSelectionPromise;
  } finally {
    modelSelectionPromise = undefined;
  }
}

async function doSelectModel(): Promise<vscode.LanguageModelChat | undefined> {
  if (!vscode.lm || !vscode.lm.selectChatModels) {
    return undefined;
  }

  outputChannel.appendLine('[LLM Proxy] Selecting chat models...');

  let models = await vscode.lm.selectChatModels({ vendor: 'copilot', family: 'gpt-4o' });
  outputChannel.appendLine(`[LLM Proxy] gpt-4o models found: ${models.length}`);

  if (models.length === 0) {
    models = await vscode.lm.selectChatModels({ vendor: 'copilot' });
    outputChannel.appendLine(`[LLM Proxy] Any Copilot models found: ${models.length}`);
  }

  if (models.length === 0) {
    models = await vscode.lm.selectChatModels();
    outputChannel.appendLine(`[LLM Proxy] Any models found: ${models.length}`);
  }

  if (models.length === 0) {
    return undefined;
  }

  cachedModel = models[0];
  outputChannel.appendLine(`[LLM Proxy] Using model: ${cachedModel.name} (${cachedModel.vendor}/${cachedModel.family})`);
  return cachedModel;
}

async function handleLLMProxyRequest(request: LLMProxyRequest): Promise<LLMProxyResponse> {
  try {
    const model = await selectModel();

    if (!model) {
      return { text: '{}', error: 'No language models available — sign in to GitHub Copilot' };
    }

    // Build messages
    const messages = [
      vscode.LanguageModelChatMessage.User(request.systemPrompt + '\n\n' + request.prompt),
    ];

    // Send the request
    const tokenSource = new vscode.CancellationTokenSource();
    const response = await model.sendRequest(messages, {}, tokenSource.token);

    // Collect the streamed response
    let text = '';
    for await (const part of response.text) {
      text += part;
    }

    return { text };
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error';
    outputChannel.appendLine(`[LLM Proxy] Error: ${message}`);
    return { text: '{}', error: `vscode.lm request failed: ${message}` };
  }
}

function isPromptDocument(document: vscode.TextDocument): boolean {
  const fileName = document.fileName.toLowerCase();
  return (
    document.languageId === 'prompt' ||
    fileName.endsWith('.prompt.md') ||
    fileName.endsWith('.system.md') ||
    fileName.endsWith('.agent.md') ||
    fileName.endsWith('.prompt') ||
    isSkillMarkdown(fileName)
  );
}

function isSkillMarkdown(fileName: string): boolean {
  return fileName.endsWith('.md') && /(^|[\\/])skills[\\/]/.test(fileName);
}

export function deactivate(): Thenable<void> | undefined {
  if (!client) {
    return undefined;
  }
  return client.stop();
}
